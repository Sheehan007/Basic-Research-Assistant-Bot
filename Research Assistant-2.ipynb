{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b56f9abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing chunk 1/5...\n",
      "Summarizing chunk 2/5...\n",
      "Summarizing chunk 3/5...\n",
      "Summarizing chunk 4/5...\n",
      "Summarizing chunk 5/5...\n",
      "\n",
      "--- SUMMARY ---\n",
      "\n",
      "new technologies such as artificial intelligence (AI) have the intrinsic ability to gain insights from large amounts of data from various sources and may be used to solve these problems . medical professionals are overwhelmed by data from infu- sion pumps, vital sign monitors, laboratory tests, molecular tests, medical images .\n",
      "digitized medical records will double itself every 73 days by 2020 . a doctor would need to spend 29 hours a day absorbing new medical knowledge to stay up to date . this development will bring disruptive change to how we research, develop, approve and pay for medicines .\n",
      "high-quality data from a wide array of sources can be collected for each patient and can be connected to data from large pools of other patients for analysis [6], [7]. this enables us to arrive at a deeper understanding of disease biology and its expression in individual patients [8]. patients are more knowledgeable and informed, and in the position to demand innovative and effec-tive treatments .\n",
      "a subcategory of AI can combine data from all state-of-the-art diagnostic tests and other resources . machine learning can be used to increase our understanding of important genomic pathways in lung cancer, with the use of microarray data . a key feature in the success of AI for lung cancer is that many molecular abnormalities have already been discovered .\n",
      "many algorithms to predict sepsis onset have been developed . there are no clear molecular abnor malities on which new algorithms can be trained . the potential of AI is therefore limited, as unique features needed to do adequate predictions are not yet known .\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model = \"t5-small\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with pymupdf.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean the extracted text.\"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def chunk_text(text: str, max_words: int = 300) -> List[str]:\n",
    "    \"\"\"Chunk text into manageable parts for summarization.\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "def summarize_document(text: str, max_chunks: int = 5) -> str:\n",
    "    \"\"\"Summarize the document using a smaller number of chunks.\"\"\"\n",
    "    chunks = chunk_text(text, max_words=300)[:max_chunks]  # Limit to prevent overload\n",
    "    summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {i + 1}/{len(chunks)}...\")\n",
    "        try:\n",
    "            summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            summaries.append(f\"[Error summarizing chunk {i + 1}: {e}]\")\n",
    "    return \"\\n\".join(summaries)\n",
    "\n",
    "def process_research_paper(pdf_path: str) -> str:\n",
    "    \"\"\"Process and summarize the research paper.\"\"\"\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    clean = clean_text(text)\n",
    "    return summarize_document(clean)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    paper_path = r\"C:\\\\Users\\\\SOMEN\\\\Downloads\\\\AI_Paper.pdf\"  # Adjust path as needed\n",
    "    summary = process_research_paper(paper_path)\n",
    "    print(\"\\n--- SUMMARY ---\\n\")\n",
    "    print(summary)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b745dae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SOMEN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\SOMEN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the Research Paper\n",
      "Summarizing chunk 1/3...\n",
      "Done in 22.94s \n",
      "Summarizing chunk 2/3...\n",
      "Done in 19.32s \n",
      "Summarizing chunk 3/3...\n",
      "Done in 27.64s \n",
      "\n",
      "---SUMMARY---\n",
      "\n",
      "Post-Training Quantization (PTQ) is a technique for compressing neural networks into very few bits. The proposed method compensates for the errors resulting from quantization, and it improves accuracy in 2/4-bit quantization with less than 1.5% accuracy. The method is valid on CNNs and extends to ViT and object detection models.\n",
      "The method of quantization used by G. Choi and M. Moaiyeri is suitable for real-time edge deployment. It reduces memory demands and computational costs in deep learning models. It is particularly advantageous for compressing large models or when access to the original training data is limited.\n",
      "Weight-only quantization is the most computationally intensive method in low-bit PTQ. To address this, we introduce new parameters in PTQ that adjust the scale and bias on channel-by-channel basis to align the model with the original model.\n",
      "\n",
      "Question: Who are the authors? \n",
      "Answer: G. Choi\n",
      "\n",
      "Question: What is the paper about?\n",
      "Answer: neural network quantization\n",
      "\n",
      "Question: How can it benefit the world?\n",
      "Answer: enhancing performance without substantial hardware modifications\n",
      "\n",
      "Question: Which field of engineering is it relevant to?\n",
      "Answer: electrical engineering\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "import time \n",
    "\n",
    "# Defining the models\n",
    "summarizer = pipeline(\"summarization\", model = \"knkarthick/MEETING_SUMMARY\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model = \"deepset/roberta-base-squad2\")\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with pymupdf.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean the extracted text.\"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def chunk_text(text: str, max_words: int = 300) -> List[str]:\n",
    "    \"\"\"Chunk text into manageable parts for summarization.\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "def summarize_document(text: str, max_chunks: int = 3) -> str:\n",
    "    \"\"\"Summarize the document using a smaller number of chunks.\"\"\"\n",
    "    chunks = chunk_text(text, max_words=300)[:max_chunks]  # Limit to prevent overload\n",
    "    summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {i + 1}/{len(chunks)}...\")\n",
    "        start = time.time() \n",
    "        try:\n",
    "            summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "            print(f\"Done in {round(time.time() - start, 2)}s \")\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            summaries.append(f\"[Error summarizing chunk {i + 1}: {e}]\")\n",
    "    return \"\\n\".join(summaries)\n",
    "\n",
    "\n",
    "def ask_questions(text:str, question:str) -> str:\n",
    "    \"\"\"Answer a question using HuggingFace QA Model.\"\"\"\n",
    "    try:\n",
    "        answer = qa_pipeline(question=question, context=text)\n",
    "        return answer[\"answer\"]\n",
    "    except Exception as e:\n",
    "        return f\"[QA failed: {e}]\"\n",
    "\n",
    "    \n",
    "# Integrating Command-Line Interface (CLI)\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        pdf_path = r\"C:\\\\Users\\\\SOMEN\\\\Downloads\\\\Quant_Paper.pdf\"\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description = \"AI Research Paper Assistant\")\n",
    "        parser.add_argument(\"pdf_path\", help = \"Path to the research paper PDF\")\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    print(\"Processing the Research Paper\")\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    cleaned = clean_text(full_text)\n",
    "    summary = summarize_document(cleaned)\n",
    "\n",
    "    print (\"\\n---SUMMARY---\\n\")\n",
    "    print (summary)\n",
    "\n",
    "# Interactive QA\n",
    "    while True: \n",
    "        user_question = input(\"\\n Ask a question about the paper or type 'exit' to quit\\n\")\n",
    "        if user_question.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        answer = ask_questions(cleaned, user_question)\n",
    "        print (f\"\\nQuestion: {user_question}\")\n",
    "        print (f\"Answer: {answer}\")\n",
    "\n",
    "        with open(\"qa_log.txt\", \"a\", encoding = \"utf-8\") as f:\n",
    "            f.write(f\"\\nQ:{user_question}\\nA:{answer}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fae14b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForQuestionAnswering: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForQuestionAnswering for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the Research Paper...\n",
      "Word count: 534\n",
      "Summarizing chunk 1/2...\n",
      "Done in 142.33s\n",
      "Summarizing chunk 2/2...\n",
      "Done in 62.97s\n",
      "\n",
      "--- SUMMARY ---\n",
      "\n",
      "Post-Training Quantization (PTQ) is a technique for compressing neural networks into very few bits. The proposed method compensates for the errors resulting from quantization, and it improves accuracy in 2/4-bit quantization with less than 1.5%. The method is valid on CNNs and extends to ViT and object detection models.\n",
      "Mohammad Hossein Moaiyeri approved the manuscript. Quantization is a model compression technique that significantly reduces memory demands and computational costs in deep learning models. QAT trains a model with quantized weights and activations. PTQ applies quantization to a pre-trained model.\n",
      "Answer: Post-training quantization\n",
      "Answer: Low-Bit Post-Training Quantization\n",
      "Answer: Nojun Kwak\n",
      "Answer: Post-training quantization\n",
      "Answer: Seoul National University\n",
      "Answer: NRF funded by MSIT of Korean Government\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "import time \n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    text = \"\"\n",
    "    with pymupdf.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def chunk_text(text: str, max_words: int = 300) -> List[str]:\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "def summarize_document(text: str, max_chunks: int = 3) -> str:\n",
    "    chunks = chunk_text(text, max_words=300)[:max_chunks]\n",
    "    summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {i + 1}/{len(chunks)}...\")\n",
    "        start = time.time()\n",
    "        try:\n",
    "            summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "            print(f\"Done in {round(time.time() - start, 2)}s\")\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            summaries.append(f\"[Error summarizing chunk {i + 1}: {e}]\")\n",
    "    return \"\\n\".join(summaries)\n",
    "\n",
    "def ask_questions(text: str, question: str) -> str:\n",
    "    try:\n",
    "        answer = qa_pipeline(question=question, context=text)\n",
    "        return answer[\"answer\"]\n",
    "    except Exception as e:\n",
    "        return f\"[QA failed: {e}]\"\n",
    "\n",
    "# CLI \n",
    "if __name__ == \"__main__\":\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        pdf_path = r\"C:\\Users\\SOMEN\\Downloads\\Quant_Paper.pdf\"  # Adjust for local test\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description=\"AI Research Paper Assistant\")\n",
    "        parser.add_argument(\"pdf_path\", help=\"Path to the research paper PDF\")\n",
    "        args = parser.parse_args()\n",
    "        pdf_path = args.pdf_path\n",
    "\n",
    "    print(\"Processing the Research Paper...\")\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    cleaned = clean_text(full_text[:4000])\n",
    "    print(f\"Word count: {len(cleaned.split())}\")\n",
    "    summary = summarize_document(cleaned)\n",
    "\n",
    "    print(\"\\n--- SUMMARY ---\\n\")\n",
    "    print(summary)\n",
    "\n",
    "    # Interactive QA\n",
    "    while True:\n",
    "        user_question = input(\"\\n Ask a question about the paper (or type 'exit' to quit):\\n> \")\n",
    "        if user_question.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        answer = ask_questions(cleaned, user_question)\n",
    "        print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e2f7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4f526ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\somen\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\somen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395369b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fb0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
